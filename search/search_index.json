{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"StreamPU Documentation","text":""},{"location":"#introduction","title":"Introduction","text":"<p><code>StreamPU</code> is a DSEL for streaming applications written in C++. This  documentation maily focuses on explaining the basic elements of the language  from the developer point of view. The user documention is is currently  underway.</p> <p>Here are the main features of <code>StreamPU</code>:</p> <ul> <li>Definition of modules, tasks and sockets (dataflow)</li> <li>Elementary modules and tasks implementations</li> <li>Parallel runtime (replication, pipeline)</li> </ul> <p>The DSEL is suitable for SDR systems, video processing and more generally it  matches single-rate SDF streaming applications.</p> <p>Note</p> <p>This library was previously named <code>AFF3CT-core</code> as it was first extracted  from AFF3CT. Now that it is no longer  specific to channel coding and digital communications, the project has been  renamed to <code>StreamPU</code>, which is a more meaningful name.</p> <p>Note</p> <p>The DSEL term can be sometimes confusing and <code>StreamPU</code> can also be seen as a standard C++ library. The name \"DSEL\" comes from the ability  to write interpreted Turing-complete programs using the C++ library.</p> <p>Warning</p> <p>This library is NOT intended to address data and task parallelisms. <code>StreamPU</code> focus on replication and pipeline parallelisms. For data  parallelism, <code>StreamPU</code> combines well with OpenMP. If you look for task parallelism, using OpenMP can also be a possible solution, or other runtime  like the excellent StarPU can be a  good choice.</p>"},{"location":"#contents-of-the-documentation","title":"Contents of the Documentation","text":"<p>For Users</p> <ul> <li>Concepts:<ol> <li>Overview</li> </ol> </li> <li>Runtime:<ol> <li>Thread Pinning</li> </ol> </li> <li>Tests:<ol> <li>Bootstrap</li> <li>Advanced</li> </ol> </li> </ul> <p>For Developpers</p> <ul> <li> <p>Basic Components:</p> <ol> <li>Task</li> <li>Module</li> <li>Socket</li> <li>Sequence &amp; Subsequence</li> <li>Pipeline </li> <li>Control flow</li> </ol> </li> <li> <p>New Features and Discussions:</p> <ol> <li>Forward sockets</li> <li>Pipeline &amp; Control Flow</li> <li>Work in Progress</li> </ol> </li> </ul>"},{"location":"concepts_overview/","title":"Overview","text":""},{"location":"concepts_overview/#tasks-modules-sockets","title":"Tasks, Modules &amp; Sockets","text":"<p>TODO.</p>"},{"location":"concepts_overview/#sequences-and-pipelines","title":"Sequences and Pipelines","text":"<p>TODO.</p>"},{"location":"concepts_overview/#module-states-tasks-replicability","title":"Module States &amp; Tasks Replicability","text":"<p>In StreamPU, there are modules that are either <code>stateful</code> or <code>stateless</code>.  <code>stateful</code> means that it is an instance of a class that inherits from  <code>module::Stateful</code> class. On the other hand, a <code>stateless</code> module is necessarily  an instance of the <code>module::Stateless</code> class. The <code>module::Stateful</code> and  <code>module::Stateless</code> classes both inherit directly from the <code>module::Module</code>  class. All <code>stateless</code> modules are <code>clonable</code>. This means that, by default,  <code>stateless</code> module tasks are <code>replicable</code> (i.e. they can be replicated within a  sequence or pipeline stage to increase the system throughput). </p> <p>It gets a little bit more complex with <code>stateful</code> modules. By default, a module  of this type is <code>non-clonable</code> (i.e. it does not implement the  <code>module::Stateful::clone()</code> method). In this case, the tasks that make up the  module are, by default, <code>non-replicable</code>. However, the designer of a <code>stateful</code>  module can override the <code>module::Stateful::clone()</code> method (basically, the  designer has to expresses how to \"copy\" the module). When it is done, the module  becomes <code>clonable</code> and, automatically, the tasks that make up a <code>clonable</code>  module become <code>replicable</code>.</p> <p>Sometimes one may want to force a <code>replicable</code> task to avoid being replicated.  For instance, 1) for testing purposes or 2) sometimes certain tasks could be  replicated, but in the way they are used it makes no sense and would lead to  application malfunction. For this purpose, the public <code>runtime::Task::set_replicability(bool)</code> method gives control to the designer. If the boolean is set to <code>false</code>, then the task is forced to be  <code>non-replicable</code>.</p> <p>Warning</p> <p>If the input boolean of the <code>runtime::Task::set_replicability(bool)</code> method  is set to <code>true</code> and the corresponding module is <code>non-clonable</code>, the method  will fail and will throw an exception. In any cases, it will be possible to  set the boolean to <code>false</code> (who can do more can do less).</p> <p>To summarize:</p> <ul> <li> <p><code>stateless</code> module:</p> <ul> <li>Always <code>clonable</code></li> <li>Tasks are by default <code>replicable</code></li> <li>Tasks can be forced to be <code>non-replicable</code> by the designer</li> </ul> </li> <li> <p><code>stateful</code> module:</p> <ul> <li>By default <code>non-clonable</code></li> <li>Designer can implement the <code>module::Stateful::clone()</code> method to make the    module <code>clonable</code></li> <li>By default, if the <code>module::Stateful::clone()</code> method is not implemented,   tasks are <code>non-replicable</code></li> <li>If the <code>module::Stateful::clone()</code> method is implemented, tasks become   <code>replicable</code> and the designer can set the replicability to <code>true</code> or    <code>false</code></li> </ul> </li> </ul>"},{"location":"module/","title":"Module","text":"<p>A module is a container of tasks. It contains the tasks  themselves and their inner data. Inner data can be useful to avoid useless  buffers re-allocations each time a task is executed, or a to contain a state  that is updated each time a task is triggered.  Multiple tasks can be grouped  into one module and can share data through the common module (as opposed to  the use of sockets). By definition,</p> <ul> <li>a stateless task is a task that does not have any inner data     (see <code>module::Stateless</code> class),</li> <li>a stateful task is a task that have inner data and, thus, a task that    need to be part of a module. </li> </ul> <p>Each time we need to create a stateful task, we will create a new C++ class  that inherit from the <code>spu::module::Module</code> class. The <code>Module</code> class  provides <code>protected</code> methods to create new tasks and <code>public</code> methods to  manipulate the module instances.</p>"},{"location":"module/#main-attributes","title":"Main Attributes","text":"<p><pre><code>std::vector&lt;std::shared_ptr&lt;runtime::Task&gt;&gt; tasks;\n</code></pre> The tasks list of the current module. All the tasks in the vector are  allocated (no <code>nullptr</code>).</p> <p><pre><code>std::vector&lt;std::shared_ptr&lt;runtime::Task&gt;&gt; tasks_with_nullptr;\n</code></pre> The tasks list of the current module where the tasks have a fixed position in the vector. This is useful when a task is conditionally created. In the case of a task that is not created in the current module, its value is set to  <code>nullptr</code>.</p> <p><pre><code>size_t n_frames;\n</code></pre> Number of frames/streams to process each time a task is executed. For  instance, if <code>n_frames == 2</code>, all the tasks of the current module will  process 2 frames each time they are triggered.</p> <p><pre><code>std::string name;\n</code></pre> Name of the Module. This name is the same for all the instances of one class.</p> <p><pre><code>std::string short_name;\n</code></pre> Short name of the Module. This name is the same for all the instances of one  class.</p> <p><pre><code>std::string custom_name;\n</code></pre> Custom name of the Module. This name can be redefined by the user for each  instance.</p>"},{"location":"module/#main-protected-methods","title":"Main Protected Methods","text":"<p><pre><code>void set_name(const std::string &amp;name);\n</code></pre> Sets the module name.</p> <p><pre><code>void set_short_name(const std::string &amp;short_name);\n</code></pre> Sets the module short name.</p> <p><pre><code>runtime::Task&amp; create_task(const std::string &amp;name, const int id = -1);\n</code></pre> Creates a new task, two tasks cannot share the same <code>name</code>.</p> <p><pre><code>template &lt;typename T&gt;\nsize_t create_socket_in(runtime::Task&amp; task, const std::string &amp;name, const size_t n_elmts);\n</code></pre> Creates an input socket over a given task.</p> <p><pre><code>template &lt;typename T&gt;\nsize_t create_socket_out(runtime::Task&amp; task, const std::string &amp;name, const size_t n_elmts);\n</code></pre> Creates an output socket over a given task.</p> <p><pre><code>template &lt;typename T&gt;\nsize_t create_socket_fwd(runtime::Task&amp; task, const std::string &amp;name, const size_t n_elmts);\n</code></pre> Creates a forward socket over a given task.</p> <p><pre><code>void create_codelet(runtime::Task&amp; task, std::function&lt;int(module::Module &amp;m, runtime::Task &amp;t, onst size_t frame_id)&gt; codelet);\n</code></pre> Creates the codelet of the given task.</p>"},{"location":"module/#main-public-methods","title":"Main Public Methods","text":"<p><pre><code>size_t get_n_frames() const;\n</code></pre> Returns the number of frames to process in this Module.</p> <p><pre><code>void set_n_frames(const size_t n_frames);\n</code></pre> Sets the number of frames to process each time a task is executed.</p> <p><pre><code>const std::string&amp; get_name() const;\n</code></pre> Returns the module name.</p> <p><pre><code>const std::string&amp; get_short_name() const;\n</code></pre> Returns the module short name.</p> <p><pre><code>void set_custom_name(const std::string &amp;custom_name);\n</code></pre> Sets the module custom name (each instance can have a different custom name).</p> <p><pre><code>const std::string&amp; get_custom_name() const;\n</code></pre> Gets the custom name.</p> <p><pre><code>runtime::Socket&amp; operator[](const std::string &amp;tsk_sck);\n</code></pre> Returns the socket if it exists. The expected string format is  <code>\"task_name::socket_name\"</code>.</p> <p><pre><code>runtime::Task&amp; operator()(const std::string &amp;tsk_name);\n</code></pre> Returns the task if it exists. The input string has to match an existing task  name in this module.</p>"},{"location":"pipeline/","title":"Pipeline","text":"<p>A pipeline is a feature offered by <code>StreamPU</code> allowing to split a sequence into multiple stages. Each stage is executed  on one or more threads in parallel. The pipeline takes care of the  synchronizations between stages. This is achieved through an implementation  of a producer/consumer algorithm.</p> <p> </p> Example of a sequence on the left and a pipeline on the right. <p>A pipeline is a C++ object of the <code>spu::runtime::Pipeline</code> class. The  following sections try to give an overview of the most important attributes and  methods to facilitate the code understanding.</p>"},{"location":"pipeline/#main-attributes","title":"Main Attributes","text":"<p><pre><code>runtime::Sequence original_sequence;\n</code></pre> The original sequence from which the pipeline was created.</p> <p> <pre><code>std::vector&lt;std::shared_ptr&lt;runtime::Sequence&gt;&gt; stages;\n</code></pre> Vector of the different stages in the pipeline. Each stage is a sequence.</p> <p><pre><code>std::vector&lt;std::pair&lt;std::tuple&lt;runtime::Socket*, size_t, size_t, size_t,size_t&gt;,\n            std::tuple&lt;runtime::Socket*, size_t, size_t, size_t&gt;&gt;&gt; sck_orphan_binds;\n</code></pre> Vector of sockets with broken connections due to the pipeline stages creation (hence \"orphan\"). These sockets will be bound later to special modules called adaptors to make \"bridges\" between the stages.</p> <p><pre><code>std::vector&lt;std::tuple&lt;runtime::Socket*, runtime::Socket*, size_t&gt;&gt; adaptors_binds;\n</code></pre> Vector of tuple (<code>input</code>, <code>output</code>, <code>priority</code>) of the created adaptors,  <code>priority</code> is used to order the tuples.</p>"},{"location":"pipeline/#main-methods","title":"Main Methods","text":"<p><pre><code>void exec();\n</code></pre> This is the public method that runs the pipeline in loop. Other variants exist where it is possible to give a stop condition function.</p> <pre><code>void init(const std::vector&lt;runtime::Task*&gt; &amp;firsts,\n          const std::vector&lt;runtime::Task*&gt; &amp;lasts,\n          const std::vector&lt;std::tuple&lt;std::vector&lt;runtime::Task*&gt;, std::vector&lt;runtime::Task*&gt;, std::vector&lt;runtime::Task*&gt;&gt;&gt; &amp;sep_stages = {},\n          const std::vector&lt;size_t&gt; &amp;n_threads = {},\n          const std::vector&lt;size_t&gt; &amp;synchro_buffer_sizes = {},\n          const std::vector&lt;bool&gt; &amp;synchro_active_waiting = {},\n          const std::vector&lt;bool&gt; &amp;thread_pinning = {},\n          const std::vector&lt;std::vector&lt;size_t&gt;&gt; &amp;puids = {});\n</code></pre> <p>This method creates the pipeline given:</p> <ul> <li>The first and last tasks of the original sequence (<code>firsts</code> and <code>lasts</code>).</li> <li>The first and last tasks of each stage (<code>sep_stages</code>).</li> <li>The number of threads to allocate to each stage (<code>n_threads</code>).</li> <li>The number of buffers between stages (<code>synchro_buffer_sizes</code>).</li> <li>The type of waiting for the adaptor tasks (<code>synchro_active_waiting</code>).</li> </ul> <p>Note</p> <p>StreamPU doesn't support consecutive multi-threaded stages yet.</p> <p><pre><code>void create_adaptors(const std::vector&lt;size_t&gt; &amp;synchro_buffer_sizes = {},\n                     const std::vector&lt;bool&gt; &amp;synchro_active_waiting = {});\n</code></pre> This function creates the Adaptor modules (and so the <code>pull</code> &amp; <code>push</code> tasks)  that are added between each stage to transmit data from the stage \\(S\\) to the  stage \\(S+1\\). </p> <p><pre><code>void _bind_adaptors(const bool bind_adaptors = true);\n</code></pre> Adaptor module tasks <code>pull</code> &amp; <code>push</code> need to be bound to each task in the two consecutive stages, the target sockets to bind are stored in the vector <code>sck_orphan_binds.</code></p> <p></p>"},{"location":"pipeline/#adaptor","title":"Adaptor","text":"<p><code>spu::module::Adaptor</code> is a special module automatically inserted between  stages when creating a pipeline and serve as \"bridges\" between them, they are  bound to first and last tasks of the consecutive stages. The purpose of adaptors  is to synchronize data exchange between each stage using pre-allocated buffer  pools. In other words, this is an implementation of the producer-consumer  algorithm. There are 4 tasks performed by adaptors:</p> <ul> <li><code>push_1</code>: when the \\(S\\) stage is executed on one thread and the \\((S+1)\\) stage    is executed on multiple threads. The function gets an empty buffer and fills    it with the data produced in the stage \\(S\\). The buffers are filled using a    round-robin algorithm.</li> <li><code>pull_n</code>: when the \\(S\\) stage is executed on multiple threads and the \\((S-1)\\)   stage is on one thread. It is the task executed just after the <code>push_1</code>, it   takes a filled buffer from the inter-stage pool and forwards the data. There    is a <code>pull_n</code> task for every thread of the stage.</li> <li><code>push_n</code>: when the \\(S\\) stage is executed on multiple threads and the \\((S+1)\\)   stage is on one thread. The task takes an empty buffer from the pool and fills   it with the data produced by the thread. There is a <code>push_n</code> task for each   thread of the stage.</li> <li><code>pull_1</code>: when the \\(S\\) stage is executed on one thread and the \\((S-1)\\) stage    is on multiple threads, it's the task executed just after the <code>push_n</code>. It    takes filled buffers from the pool using the same round-robin algorithm as    <code>push_1</code> and forward the data.</li> </ul>"},{"location":"pipeline/#main-attributes_1","title":"Main Attributes","text":"<p><pre><code>const size_t buffer_size;\n</code></pre> The inter-stage buffer pool size.</p> <p><pre><code>std::shared_ptr&lt;std::vector&lt;std::vector&lt;std::vector&lt;int8_t*&gt;&gt;&gt;&gt; buffer;\n</code></pre> Pointers to each buffer of the inter-stage pool.</p> <p><pre><code>std::shared_ptr&lt;std::vector&lt;std::atomic&lt;uint64_t&gt;&gt;&gt; first;\nstd::shared_ptr&lt;std::vector&lt;std::atomic&lt;uint64_t&gt;&gt;&gt; last;\n</code></pre> Two pointers used to monitor the buffer pool, <code>first</code> is used to get the filled buffers, and <code>last</code> for the empty ones.</p>"},{"location":"pipeline/#main-methods_1","title":"Main Methods","text":"<p>These are the methods used to synchronize the buffer pool between the pipeline stages. When getting a buffer, the thread may sleep if there is no empty  buffer available. When a new empty buffer will be available, the sleeping  thread will be woken up.</p> <p><pre><code>void* get_empty_buffer(const size_t sid);\n</code></pre> Get a pointer to the first empty buffer in the pool (at index <code>last</code>).</p> <p><pre><code>void* get_filled_buffer(const size_t sid);\n</code></pre> Get a pointer to the first filled buffer in the pool (at index <code>first</code>).</p> <p><pre><code>void* get_empty_buffer(const size_t sid, void* swap_buffer);\n</code></pre> Get a pointer to the first empty buffer in the pool, and replace this buffer with a new one pointed by <code>swap_buffer</code> parameter. </p> <p><pre><code>void* get_filled_buffer(const size_t sid, void* swap_buffer);\n</code></pre> Get a pointer to the first filled  buffer in the pool, and replace this buffer with a new one pointed by <code>swap_buffer</code> parameter.</p> <p><pre><code>void wake_up_pusher();\n</code></pre> The puller can wake up a push task if this one is waiting for an empty buffer.</p> <p><pre><code>void wake_up_puller();\n</code></pre> The pusher can wake up a pull task if this one is waiting for an empty buffer.</p>"},{"location":"pipeline_ctrl_flow/","title":"Pipeline &amp; Control Flow","text":""},{"location":"pipeline_ctrl_flow/#introduction","title":"Introduction","text":"<p><code>StreamPU</code> allows the user to control the execution flow of sequences through the use of switcher, however sequences are often used within the context of pipelines and thus some slight behavior adjustments  were required for them to work consistently.</p>"},{"location":"pipeline_ctrl_flow/#technical-improvements","title":"Technical Improvements","text":""},{"location":"pipeline_ctrl_flow/#finding-the-last-sub-sequence","title":"Finding the Last Sub-sequence","text":"<p>Upon creation, a pipeline must add <code>pull</code> and <code>push</code> tasks at the beginning and  the end of the sequences making up its stages (see pipeline  section and sequence section for the relationship between the  two). For that purpose, a DFS algorithm is used to traverse the  directed graph starting from the root of the sequence,  marking every node on the path and returning the last node it passed through.  This however can return incorrect nodes depending on the configuration of the  sequence.</p> <p>Pseudo-code <pre><code>Node Last_Subseq(Node n):\n    mark(n)\n    Node last_node = n\n    for every child c of n that is not marked:\n        last_node = Last_Subseq(c)\n    return last_node\n</code></pre></p>"},{"location":"pipeline_ctrl_flow/#dfs-for-the-last-sub-sequence","title":"DFS for the Last Sub-sequence","text":"SwitchLoopNo switcher <p><pre><code>    graph LR;\n    A(SS 1)--&gt;B(SS commute);\n    B(SS commute)-.-&gt;C(SS Branch 1);\n    B(SS commute)-.-&gt;D(SS Branch 2);\n    B(SS commute)-.-&gt;E(SS Branch 3);\n    C(SS Branch 1)-.-&gt;F(SS select);\n    D(SS Branch 2)-.-&gt;F(SS select);\n    E(SS Branch 3)-.-&gt;F(SS select);\n    F(SS select)--&gt;G(SS 2);</code></pre> Here are the paths the DFS would take are</p> <ul> <li>[<code>SS 1</code>, <code>SS commute</code>, <code>SS Branch 1</code>, <code>SS select</code>, <code>SS 2</code>] : returns <code>SS 2</code></li> <li>[<code>SS 1</code>, <code>SS commute</code>, <code>SS Branch 2</code>] : returns <code>SS Branch 2</code></li> <li>[<code>SS 1</code>, <code>SS commute</code>, <code>SS Branch 3</code>] : returns <code>SS Branch 3</code></li> </ul> <p>As the function is recursive, it returns the result of the last path taken: <code>SS Branch 3</code>, which is incorrect, <code>SS 2</code> is the expected result.</p> <p><pre><code>graph LR;\nA(SS 1)-.-&gt;B(SS select);\nB(SS select)--&gt;C(SS 2);\nC(SS 2)--&gt;F(SS commute);\nF(SS commute)-.-&gt;E(SS 3);\nE(SS 3)-.-&gt;B(SS select);\nF(SS commute)-.-&gt;G(SS 4);</code></pre> Here are the paths the DFS would take are</p> <ul> <li>[<code>SS 1</code>, <code>SS select</code>, <code>SS 2</code>, <code>SS commute</code>, <code>SS 3</code>] : returns <code>SS 3</code></li> <li>[<code>SS 1</code>, <code>SS select</code>, <code>SS 2</code>, <code>SS commute</code>, <code>SS 4</code>] : returns <code>SS 4</code></li> </ul> <p>As the function is recursive, it returns the result of the last path taken: <code>SS 4</code>, which is correct, but deceptive. It only happened to work because of the order in which the children of node <code>SS commute</code> were parsed. If <code>SS 4</code> had been parsed first, then it would have returned <code>SS 3</code>. This kind of behavior is problematic as the algorithm should not depend on which children is first in a list, as that is not relevant to the layout of the graph.</p> <p><pre><code>graph LR;\nA(SS 1);</code></pre> As explained in the sequence section, a sequence with no  switcher would only have a single sub-sequence, thus the DFS  would return <code>SS 1</code> as the last sub-sequence which is correct.</p>"},{"location":"pipeline_ctrl_flow/#improved-dfs","title":"Improved DFS","text":"<p>The solution is to consider the node without children as the last one. <pre><code>Node Last_Subseq(Node n):\n    mark(n)\n    if n is childless:\n        return n\n    else\n        Node last_node = null\n        for every child c of n that is not marked:\n            Node branch_result = Last_Subseq(c)\n            if branch_result != null:\n                last_node = branch_result\n        return last_node\n</code></pre></p> <p>This is simple and efficient.</p> <p>Info</p> <p>This is the current implementation in <code>StreamPU</code>.</p>"},{"location":"pipeline_ctrl_flow/#finding-invalid-switchers","title":"Finding Invalid Switchers","text":"<p>Another use of the DFS algorithm is to notify the user of improper uses of  switchers. <code>commute</code> and <code>select</code> tasks must always have paths linking each  other. We find broken paths by traversing the sub-sequences with a modified DFS.  Since the DFS already records parsed nodes we can use this information to tell  if a <code>commute</code> or <code>select</code> task is orphan (and thus, invalid).</p>"},{"location":"pipeline_ctrl_flow/#depth-first-search-for-invalid-switchers","title":"Depth-first Search for Invalid Switchers","text":"<p>The following sub-sequences denotes an invalid binding.</p> <pre><code>    graph LR;\n    A(SS 1)--&gt;B(SS commute);\n    B(SS commute)-.-&gt;C(SS Branch 1);\n    B(SS commute)-.-&gt;D(SS Branch 2);\n    B(SS commute)-.-&gt;E(SS Branch 3);\n    C(SS Branch 1)-.-&gt;F(SS select);\n    D(SS Branch 2)-.-&gt;F(SS select);\n    F(SS select)--&gt;G(SS 2);</code></pre> <p>This sub-sequence is invalid because the last <code>SS Branch 3</code> has no path to the  <code>select</code> task. Here are the paths the DFS would take:</p> <ul> <li>[<code>SS 1</code>, <code>SS commute</code>, <code>SS Branch 1</code>, <code>SS select</code>, <code>SS 2</code>] : No problem, the    list contains both commute and select</li> <li>[<code>SS 1</code>, <code>SS commute</code>, <code>SS Branch 2</code>, <code>SS select</code>, <code>SS 2</code>] : Ditto</li> <li>[<code>SS 1</code>, <code>SS commute</code>, <code>SS Branch 3</code>] : Invalid, this path only contains a    <code>commute</code>. We notify the user regarding the broken <code>commute</code>.</li> </ul> <pre><code># Note that path_taken here is copied between recursive calls and NOT shared\nvoid Check_ctrl_flow(Node n, List path_taken):\n    if n is not in path_taken and n is not childless\n        path_taken.append(n)\n        for every child c of n:\n            Check_ctrl_flow(c, path_taken)\n    else\n        for i = 0, i &lt; path_taken.size, i++:\n            if path_taken[i] does not contain a switcher task:\n                continue:\n            Task first_task  = path_taken[i] #We found the first task\n            for j = i, j &lt; path_taken.size, j++:\n                # We found the second task\n                if path_taken[j] is the opposite switcher task of path_taken[i]: \n                    break:\n            # We went through the entire path and didn't find the other switcher \n            # task\n            if j == path_taken.size \n                throw an error\n</code></pre> <p>Danger</p> <p>Edit 2024-03-31: We found that the previous check algorithm is not  valid in the general case. For instance, in the case of nested do while loops, it will raise an error when there is none. Thus, this check has been disabled until a more robust solution can be found.</p>"},{"location":"pipeline_ctrl_flow/#tests","title":"Tests","text":"<p>Some specific tests have been added to the project to validate the robustness of  the control flow inside a pipeline stage.</p> Switch-case inside a parallel stageNested loops inside a parallel stage <p> <code>test-exclusive-paths-pipeline</code>. <pre><code>test-exclusive-paths-pipeline -t 4 -i ../CMakeLists.txt\n</code></pre> In this test, the read bytes (from the source \\(t_1\\)) are alternatively  converted to upper case and to lower case (see \\(t_5\\) and \\(t_6\\) task). As  explained in the Work in Progress section, we  need to add a <code>relay</code> task (\\(t_8\\)) after the <code>select</code> task (\\(t_7\\)) to make it work.</p> <p> <code>test-nested-loops-pipeline</code>. <pre><code>test-nested-loops-pipeline -i 20 -j 50 -t 8\n</code></pre> In this test, two nested loops inside a parallel stage are tested. <code>-i</code> sets the number of iterations in the outer loop (\\(t_4\\) Iterator) and  <code>-j</code> sets the number of iterations in the inner loop (\\(t_7\\) Iterator). As explained in the Work in Progress section, we  need to add <code>relay</code> tasks (\\(t_2\\) and \\(t_{15}\\)) before the first <code>select</code>  task (\\(t_3\\)) and after the last <code>commute</code> task (\\(t_5\\)) to make it work.</p>"},{"location":"sequence/","title":"Sequence","text":""},{"location":"sequence/#sequence","title":"Sequence","text":"<p>A sequence is a set of bound tasks. It represents the graph to  execute for each new frame (= new stream). When a sequence is built, the tasks  execution order is fixed. Thus, for each frame, the graph is executed following  a fixed \"sequence\" of tasks.</p> <p> </p> Example of a simple sequence of tasks (single threaded). <p>A sequence is a C++ object of the <code>spu::runtime::Sequence</code> class. The  following sections try to give an overview of the most important attributes and  methods to facilitate the code understanding.</p>"},{"location":"sequence/#main-attributes","title":"Main Attributes","text":"<p><pre><code>size_t n_threads;\n</code></pre> The number of threads that are executing the sequence.</p> <p><pre><code>std::vector&lt;tools::Digraph_node&lt;runtime::Sub_sequence&gt;*&gt; sequences;\n</code></pre> Vector of sub-sequences of the main sequence (one per thread).</p> <p><pre><code>std::vector&lt;size_t&gt;                      firsts_tasks_id;\nstd::vector&lt;size_t&gt;                      lasts_tasks_id;\nstd::vector&lt;std::vector&lt;runtime::Task*&gt;&gt; firsts_tasks;\nstd::vector&lt;std::vector&lt;runtime::Task*&gt;&gt; lasts_tasks;\n</code></pre> Vectors used to get the firsts and lasts tasks of the sequence. The first tasks are the ones without parents, and  the last are the ones without children in the constructed directed graph.</p> <p><pre><code>std::vector&lt;std::vector&lt;module::Module*&gt;&gt; all_modules;\n</code></pre> Vector of modules contained within the sequence.</p>"},{"location":"sequence/#main-methods","title":"Main Methods","text":"<p><pre><code>void exec();\n</code></pre> This is the public method that runs the sequence in loop. Other variants exist where it is possible to give a stop condition function.</p> <p><pre><code>void gen_processes(const bool no_copy_mode = false);\n</code></pre> This function is one of the most important of the sequence class, it is called by the <code>Sequence</code> constructor. Its main purpose is to parse the sub-sequence  graph and to perform some operations that can modify the user bindings.  Additionally, some tasks can be optimized and/or interpreted as a DSEL keyword. </p> <p>Warning</p> <p>Before reading the following paragraph you should be familiar with the  Adaptor and Switcher modules.</p> <p>Here is a list of the transformations that are performed during the  <code>gen_processes</code> method:</p> <ul> <li><code>push</code> &amp; <code>pull</code> tasks (from <code>Adaptor</code> module): as explained in the     adaptor's section, tasks change their <code>dataptr</code> when     they get the new buffers from the inter-stage pool, the new pointer needs to     be updated for each socket bound to the old one. This behavior is added     through a <code>process</code> (nothing to do with OS processes) that encapsulates <code>push</code>     and <code>pull</code> tasks. This <code>process</code> is triggered each time there is a <code>pull</code> or    <code>push</code> task execution in the sequence.</li> <li><code>commute</code> &amp; <code>select</code> tasks (from <code>Switcher</code> module): these two tasks are used     to select which path to flow for the execution, when a path is selected the     bound sockets need to update their <code>dataptr</code> to follow the right one. Same     as before, a dedicated <code>process</code> is created and triggered.</li> <li>Other tasks: a dumb <code>process</code> will be created for each task and it will only     call its corresponding task.</li> </ul> <p> <pre><code>void explore_thread_rec(Socket* socket, std::vector&lt;runtime::Socket*&gt;&amp; list_fwd);\n</code></pre> The function is called by <code>gen_processes</code> to get all the bound sockets (next) of the modified one, if the encountered socket is of type <code>forward</code> the function is called recursively on this new socket (see the Forward socket and pipeline section). This call is performed  once at sequence build.</p> <p>Warning</p> <p>This function is no longer a method of the <code>runtime::Sequence</code> class, it is  now integrated as an anonymous (= lambda) function in the <code>gen_processes</code>  method. This is because <code>explore_thread_rec</code> is only used in  <code>gen_processes</code>.</p> <p> <pre><code>void explore_thread_rec_reverse(runtime::Socket* socket, std::vector&lt;runtime::Socket*&gt;&amp; list_fwd);\n</code></pre> The function does the same thing as the previous one, but in the other sense (previous).</p> <p>Warning</p> <p>This function is no longer a method of the <code>runtime::Sequence</code> class, it is  now integrated as an anonymous (= lambda) function in the <code>gen_processes</code>  method. This is because <code>explore_thread_rec_reverse</code> is only used in  <code>gen_processes</code>.</p> <p></p>"},{"location":"sequence/#sub-sequence","title":"Sub-sequence","text":"<p>When control flow tasks are introduced into a sequence, the execution is not only defined by the tasks binding but also by their output  sockets. For this purpose, tasks are grouped into sub-sequences. Sub-sequences  are organized in a directed graph with two nodes designated as begin  and end, respectively. This graph is recursively built during a sequence  initialization from the first task and going from bound <code>output</code>/<code>forward</code>  socket to bound <code>input</code>/<code>forward</code> socket. When a control flow task (<code>select</code> or  <code>commute</code>) is reached, a new control flow node is created and new children nodes  for each of its paths. Only a single of those paths can be taken during  execution, hence why they are referred to as exclusive paths. This also  means that a sequence with no control flow task will always have a single  sub-sequence, because it has a single path.</p> <p>Upon execution the sequence will iterate over its sub-sequences and execute  every task they contain, if one of those tasks happens to be a <code>commute</code> it will select the children node designated by its path attribute, thus branching in the execution.</p> <p><code>spu::runtime::Sub_sequence</code> (not to be confused with  <code>spu::module::Subsequence</code>!) main attributes are described in the following section.</p>"},{"location":"sequence/#main-attributes_1","title":"Main Attributes","text":"<p><pre><code>runtime::subseq_t type;\n</code></pre> The sub-sequence types can be: <code>STD</code>, <code>COMMUTE</code> and <code>SELECT</code>. This type is used  by the <code>_exec()</code> method to determine which exclusive path to take during  execution.</p> <p><pre><code>std::vector&lt;std::function&lt;const int*()&gt;&gt; processes;\n</code></pre> Whenever <code>_exec()</code> reaches a new sub-sequence it executes every function contained in this list, there is one for each task in the sub-sequence. Refer to <code>gen_processes()</code> to understand how they are created and what they contain.</p> <p><pre><code>std::vector&lt;size_t&gt; tasks_id;\n</code></pre> The ids of the tasks the <code>processes</code> were generated from, <code>tasks_id[0]</code> is the id of the task that <code>processes[0]</code> was made with.</p> <p><pre><code>size_t id;\n</code></pre> The sub-sequence's id. <pre><code>std::vector&lt;std::vector&lt;std::vector&lt;runtime::Socket*&gt;&gt;&gt; rebind_sockets;\nstd::vector&lt;std::vector&lt;std::vector&lt;void*&gt;&gt;&gt; rebind_dataptrs;\n</code></pre> This two vectors are used within the <code>gen_process()</code> method to save the sockets and their <code>dataptr</code> to update during the runtime rebinding.</p> <p></p>"},{"location":"sequence/#digraph-node","title":"Digraph Node","text":"<p>Sub-sequences make up a directed graph. Whenever a sub-sequence  is accessed, it is through this class (<code>spu::tools::Digraph_node</code>) as  sub-sequences themselves do not contain information regarding the graph.</p>"},{"location":"sequence/#main-attributes_2","title":"Main Attributes","text":"<p><pre><code>std::vector&lt;tools::Digraph_node&lt;T&gt;*&gt; parents;\nstd::vector&lt;tools::Digraph_node&lt;T&gt;*&gt; children;\n</code></pre> The nodes pointing to this node and the ones it points to respectively.</p> <p><pre><code>T* contents; /*!&lt; Pointer to the node contents, could be anything. */\n</code></pre> The contents of the node, usually a sub-sequence.</p>"},{"location":"socket/","title":"Socket","text":"<p>Sockets are used to exchange data between tasks. There are 3  different types of sockets:</p> <ul> <li>Input socket (<code>socket_t::SIN</code>): read only data,</li> <li>Output socket (<code>socket_t::SOUT</code>): write only data,</li> <li>Forward socket (<code>socket_t::SFWD</code>): read and write data.</li> </ul> <p>A task can have multiple sockets of different types (input, output and forward). This is illustrated in the following figure:</p> <p> </p> Tasks with different socket types. <p>A socket is a C++ object of the <code>spu::runtime::Socket</code> class. The following sections try to give an overview of the most important attributes and methods to facilitate the code understanding.</p>"},{"location":"socket/#main-attributes","title":"Main Attributes","text":"<p><pre><code>socket_t type;\n</code></pre> Define the socket type <code>IN</code>, <code>OUT</code> or <code>FWD</code>.</p> <p><pre><code>std::string name;\n</code></pre> Custom name for the socket.</p> <p><pre><code>std::type_index datatype;\n</code></pre> The type of data exchanged.</p> <p><pre><code>void* dataptr;\n</code></pre> Pointer to the data of the socket (memory space).</p> <p><pre><code>std::vector&lt;Socket*&gt; bound_sockets;\n</code></pre> The <code>input</code> or <code>forward</code> sockets bound to the current socket. Only relevant  for <code>output</code> or <code>forward</code> sockets.</p> <p><pre><code>Socket* bound_socket;\n</code></pre> The unique <code>output</code> or <code>forward</code> socket bound to the current socket. Only relevant for <code>input</code> or <code>forward</code> sockets.</p>"},{"location":"socket/#main-methods","title":"Main Methods","text":"<p>The most important methods of the socket class are <code>bind</code> and <code>unbind</code>.</p> <p><pre><code>void bind(Socket &amp;s_out, const int priority = -1);\n</code></pre> This function is used to connect sockets with each other, it can be called by an <code>input</code> or <code>forward</code> socket and takes as parameter an output or forward socket. The function gets the caller's <code>dataptr</code> and redirects it to <code>s_out dataptr</code>.</p> <p>Below some examples of valid and invalids socket bindings :</p> Valid bindingsInvalid bindings <p> Examples of valid socket bindings. </p> <p> Examples of invalid socket bindings. </p> <p>For invalid socket bindings, <code>StreamPU</code> will throw an exception at runtime.</p> <p><pre><code>void unbind(Socket &amp;s_out, const int priority = -1);\n</code></pre> This function is used to disconnect sockets from each other. </p> <p>Note</p> <p><code>s_out</code> must be bound to the caller socket otherwise <code>StreamPU</code> will throw an exception.</p>"},{"location":"socket/#standard-sinsout-sockets-versus-sfwd-socket","title":"Standard <code>SIN</code>/<code>SOUT</code> Sockets versus <code>SFWD</code> Socket","text":"<p>Using a couple of <code>SIN</code>/<code>SOUT</code> sockets or a single <code>SFWD</code> socket can have an  impact on the code behavior and on the performance of the application. The most  important point is the impact on the socket <code>dataptr</code> attribute.</p> <ul> <li>In the case of <code>SIN</code>/<code>SOUT</code> sockets, the input and the output sockets have    their own <code>dataptr</code>. The <code>input</code> socket receives the pointer from its bound    socket and the <code>output</code> socket has its own allocated memory space, the data    received and computed by the task are written to the <code>output</code> memory space.    The initial data are not modified in this case, there are no side effects.</li> <li>In the case of a single <code>SFWD</code> socket, the socket receives its <code>dataptr</code> from    the bound socket like an <code>input</code>. But unlike in the <code>SIN</code>/<code>SOUT</code> case, the    computed data are written directly on the provided memory space, thus    overwriting it (and potentially losing important information), there are    side effects.</li> </ul>"},{"location":"socket_fwd/","title":"Forward Socket","text":""},{"location":"socket_fwd/#introduction","title":"Introduction","text":"<p>The forward socket is a new feature added to <code>StreamPU</code> to improve the performance and the flexibility in some applications. As mentioned in the  Socket section, the <code>SFWD</code> works as an input and output at the same  time. It receives its <code>dataptr</code> from the input bound socket and this same  pointer is sent to all the output bound sockets, which means that all the  consecutive tasks bound by <code>SFWD</code> share the same memory space.</p> <pre><code>graph LR;\nA(FWD)--&gt;B(FWD); A(FWD)-.-&gt;K{MEM};\nB(FWD)--&gt;C(FWD); B(FWD)-.-&gt;K{MEM};\nC(FWD)--&gt;F(FWD); C(FWD)-.-&gt;K{MEM};\nF(FWD)-.-&gt;K{MEM};</code></pre>"},{"location":"socket_fwd/#technical-improvements","title":"Technical Improvements","text":"<p>The implementation of the forward socket for sequences was mainly  straightforward because it behaves the same way as the input and output sockets.  We just had to distinguish when it is used as an input and when it is used as an  output. However, the most challenging part was to combine forward socket with  the pipeline. Especially when forward sockets are bound from  one stage to an other.</p>"},{"location":"socket_fwd/#forward-sockets-and-pipelines","title":"Forward Sockets and Pipelines","text":"<p>As explained in the Adaptor section, a pool of buffers  is used between each stage of the pipeline. The adaptor gets a buffer from  this pool and uses it to update the output socket of its <code>pull</code> task  (<code>dataptr</code> attribute). This output socket is then bound to the input socket of  the next tasks. In other words, all the input sockets connected to the <code>pull</code>  output socket need to be updated with the new <code>dataptr</code> address.</p> <p>The forward sockets are all pointing to the same <code>dataptr</code>, so getting a new  buffer means that we have to update the <code>dataptr</code> of all the consecutive bound  forward sockets to this new memory space. In the sequence, the  same update needs to be done in the reversed way when the <code>dataptr</code> is exchanged  at the end of the stage. For that, we added two recursive methods as explained  in the sequence section (see  <code>explore_thread_rec()</code> and  <code>explore_thread_rec_reverse()</code>  methods).</p> <p>Moreover, when multiple forward sockets are crossing pipeline  stages, we need to check if these forward sockets are pointing to a same  <code>dataptr</code> or to different <code>dataptr</code>s. In the case where multiple forward sockets  are crossing pipeline stages and are pointing to the same  <code>dataptr</code>, only one buffer need to be created in the  adaptors. To detect this, a map to record the previous  <code>dataptr</code> is used (see the <code>fwd_source</code> variable in the code).</p>"},{"location":"socket_fwd/#tests","title":"Tests","text":"<p>Some specific tests have been added to the project to validate the robustness of  the forward socket implementation.</p>"},{"location":"socket_fwd/#specific-for-forward-socket","title":"Specific for Forward Socket","text":"Pipeline with two different chainsPipeline with distant stage binding (only SFWD)Pipeline with distant stage binding (SIN, SOUT &amp; SFWD)Pipeline with distant stage binding and mix of SIN, SOUT &amp; SFWD <p> <code>test-pipeline-double-chain</code>. <pre><code>test-pipeline-double-chain -t 3\n</code></pre> The purpose of this graph is to test the buffer exchange with <code>SIO</code> and <code>SFWD</code>, both on the same stage.</p> <p> <code>test-complex-pipeline-full-fwd</code>. <pre><code>test-complex-pipeline-full-fwd -t 3\n</code></pre> The purpose of this graph is to test a <code>SFWD</code> bound to two <code>SFWD</code> in two different stages, and how the buffer exchange behaves with connections between distant stages \\(S1\\) and \\(S4\\).</p> <p> <code>test-complex-pipeline-mix-fwd</code>. <pre><code>test-complex-pipeline-mix-fwd -t 3\n</code></pre> The purpose of this graph is to test a <code>SFWD</code> bound to three <code>SFWD</code> in three different stages (\\(S1 \\rightarrow S2\\), \\(S1 \\rightarrow S4\\) and \\(S1  \\rightarrow S5\\)), and how the buffer exchange behaves with connections between distant stages. Additionally, a traditional relay task (\\(t4\\) with  an input and an output socket) has been added in stage \\(S2\\). The \\(t7\\) compare task ensures that all the 3 <code>SFWD</code> have the same contents. It is expected that the final values in \\(t8\\) are \\(init + 2\\) and the final values in \\(t9\\) are \\(init + 1\\). In this test, task to task binding is used  to ensure that \\(t4\\) is executed before \\(t6\\) and \\(t8\\) is executed before  \\(t9\\) (see the oriented dashed lines).</p> <p> <code>test-complex-pipeline-inter-stage</code>. <pre><code>test-complex-pipeline-inter-stage -t 3\n</code></pre> This test is a combination of the previous tests, we have a <code>SOUT</code> bound to  a <code>SIN</code> in stage \\(S2\\) and a <code>SFWD</code> in stage \\(S4\\).</p>"},{"location":"switcher/","title":"Switcher","text":"<p>A switcher is a control flow module used to break sequences  into exclusive paths through its two tasks: <code>select</code> and  <code>commute</code>.  </p> <p>A switcher is a C++ object of the <code>spu::module::Switcher</code> class. The  following sections try to give an overview of the most important attributes and  methods to facilitate the code understanding.</p>"},{"location":"switcher/#main-attributes","title":"Main Attributes","text":"<p> <pre><code>size_t path;\n</code></pre> The exclusive path to take when the <code>commute</code> task is reached. Read on the <code>ctrl socket</code> of the <code>commute</code> task each time it is executed. The  initial <code>path</code> value is set to <code>n_data_sockets - 1</code> prior to the first  execution.</p> <p><pre><code>const size_t n_data_sockets;\n</code></pre> The number of data sockets for the <code>commute</code> and <code>select</code> tasks (not the total,  but the individual number).</p> <p><pre><code>const std::type_index datatype_commute;\nconst std::type_index datatype_select;\n</code></pre> The type of data conveyed by the data sockets of each task.</p> <p><pre><code>const size_t n_elmts_commute;\nconst size_t n_elmts_select;\n</code></pre> The number of elements conveyed by each data sockets. With <code>datatype</code> they define what and how much each data socket is expected to read/write.</p> <p><pre><code>const size_t n_bytes_commute;\nconst size_t n_bytes_select;\n</code></pre> The product of the size  of <code>datatype</code> with <code>n_elemts</code> for the total number of bytes expected on each data socket.</p>"},{"location":"switcher/#tasks","title":"Tasks","text":"<p>Since those tasks have a variable number of sockets they are accessed through the subscript <code>operator[]</code> with a numerical index or a <code>std::string</code> unlike  regular tasks which use namespaces and enumerators.</p> <p>Examples:</p> Numerical indexes<code>std::string</code> <pre><code>Switcher swi(2, 6, typeid(uint8_t)); // n_data_sockets, n_elemts, datatype\n\nswi[module::swi::tsk::select ][0]; // input  socket data0\nswi[module::swi::tsk::select ][1]; // input  socket data1\nswi[module::swi::tsk::select ][2]; // output socket data\nswi[module::swi::tsk::select ][3]; // output socket status\n\nswi[module::swi::tsk::commute][0]; // input  socket data\nswi[module::swi::tsk::commute][1]; // input  socket ctrl\nswi[module::swi::tsk::commute][2]; // output socket data0\nswi[module::swi::tsk::commute][3]; // output socket data1\nswi[module::swi::tsk::commute][4]; // output socket status\n</code></pre> <pre><code>Switcher swi(2, 6, typeid(uint8_t)); // n_data_sockets, n_elemts, datatype\n\nswi[ \"select::in_data0\" ];         // input  socket data0\nswi[ \"select::in_data1\" ];         // input  socket data1\nswi[ \"select::out_data\" ];         // output socket data\nswi[ \"select::status\"   ];         // output socket status \n\nswi[\"commute::in_data\"  ];         // input  socket data\nswi[\"commute::in_ctrl\"  ];         // input  socket ctrl\nswi[\"commute::out_data0\"];         // output socket data0\nswi[\"commute::out_data1\"];         // output socket data1\nswi[\"commute::status\"   ];         // output socket status\n</code></pre> <p></p>"},{"location":"switcher/#commute","title":"Commute","text":"<p>The <code>commute</code> task is used to create exclusive paths. When the <code>commute</code>  task is executed it reads the <code>path</code> to take from its <code>ctrl</code> socket and  then copies the bytes from its <code>data</code> input socket to the <code>data{path}</code> output  socket. Then it sets the path attribute of the module to the one read.</p> <p>Any task bound to its output sockets before <code>status</code> will be considered in a different exclusive path.</p> <p>Sockets</p> Type Name Index <code>Input</code> data 0 <code>Input</code> ctrl 1 <code>Output</code> data{0..N-1} {2..N+1} <code>Output</code> status N+2 <p></p>"},{"location":"switcher/#select","title":"Select","text":"<p>The <code>select</code> task is used to join exclusive paths.</p> <p>When a <code>select</code> task is executed it gets the <code>path</code> from the module and copies the bytes from its <code>data{path}</code> input to its <code>data</code> output socket. Any  task bound to its input sockets will be considered in a different exclusive path.</p> <p>Danger</p> <p>Note that the execution will fail if the <code>path</code> it was executed from does not match the one in the module. That is most likely to happen in  a loop because the <code>select</code> is executed before the <code>commute</code>.  Then the path defaults to <code>n_data_sockets - 1</code> meaning that in a loop, the  first path taken before the first <code>select</code> execution should always be bound  to the last input socket.</p> <p>Sockets</p> Type Name Index <code>Input</code> data{0..N-1} {0..N-1} <code>Output</code> data N <code>Output</code> status N+1 <p>"},{"location":"switcher/#examples","title":"Examples","text":"While loopSwitch-case <p> Example of a sequence with a while-loop. </p> <p> Example of a sequence with a switch-case. </p>"},{"location":"task/","title":"Task","text":"<p>A task represents the code executed by a node in the data flow graph. In  other languages, a task can be referred to as a job or a filter.  A task is defined by its input and output data and the code to execute when  triggered. In other words, a task comes with a set of data called  sockets (not to be confused with network and system sockets). The  sockets model the data that are consumed (input socket) and produced (output  socket) by the current task. Finally, the code to execute is stored in a  so-called codelet.</p> <p>A task is a C++ object of the <code>spu::runtime::Task</code> class. The following sections try to give an overview of the most important attributes and methods to facilitate the code understanding.</p>"},{"location":"task/#main-attributes","title":"Main Attributes","text":"<p><pre><code>std::vector&lt;std::shared_ptr&lt;runtime::Socket&gt;&gt; sockets;\n</code></pre> The list of sockets that are attached to this task.</p> <p><pre><code>std::vector&lt;std::vector&lt;uint8_t&gt;&gt; out_buffers;\n</code></pre> The allocated data of the output sockets of this task. If the <code>autoalloc</code>  attribute is set to <code>true</code> (see below) then the data are allocated here,  otherwise this vector is left empty.</p> <p><pre><code>std::function&lt;int(module::Module &amp;m, runtime::Task&amp; t, const size_t frame_id)&gt; codelet;\n</code></pre> The function called by <code>_exec()</code> method (see below), thus dictating the  task's behavior. Usually set in the module's constructor and should  return a <code>status_t</code>.</p> <p><pre><code>std::shared_ptr&lt;runtime::Socket&gt; fake_input_sockets;\n</code></pre> Fake input sockets are used when specifying dependencies between tasks directly. Thus, internally, these dependencies are managed through \"fake input sockets\" that are created on-the-fly over the current task. The data of these sockets are ignored during the codelet execution.</p> <p><pre><code>bool autoalloc;\n</code></pre> If set to <code>true</code>, let <code>StreamPU</code> allocate and reallocate memory needed by  the task. Data are only allocated in the output sockets. By default this  attribute is set to <code>true</code>.</p> <p><pre><code>bool stats;\n</code></pre> If <code>true</code>, records statistics regarding the task's execution, such as the  <code>duration</code>. By default this attribute is set to <code>false</code>.</p> <p><pre><code>bool fast;\n</code></pre> If <code>true</code>, skips <code>can_exec()</code> runtime check, thus, improving performance.  Sockets bound to this task will also be set to <code>fast</code>. By default this  attribute is set to <code>false</code>.</p> <p><pre><code>bool debug;\n</code></pre> If set to true, displays the task's sockets data and its status upon  execution (on the standard output). By default this attribute is set to  <code>false</code>.</p> <p><pre><code>module::Module *module;\n</code></pre> A pointer to the corresponding module. See the Module section for  more information about what is a module.</p> <p><pre><code>const std::string name;\n</code></pre> A name to identify the task. This name is unique in the module.</p>"},{"location":"task/#main-methods","title":"Main Methods","text":"<p><pre><code>const std::vector&lt;int&gt;&amp; exec(const int frame_id = -1, const bool managed_memory = true);\n</code></pre> Calls <code>_exec()</code> method, records execution statistics (if <code>stats == true</code>) and  prints the debug logs (if <code>debug == true</code>).</p> <p><pre><code>void _exec(const int frame_id = -1, const bool managed_memory = true);\n</code></pre> Executes the task's <code>codelet</code> and sets the <code>status</code> for this specific call. Called by <code>exec()</code> (see the above method).</p> <p><pre><code>bool can_exec() const;\n</code></pre> Returns <code>true</code> if all the sockets are associated to an allocated buffer,  otherwise returns <code>false</code>. Called by <code>exec()</code> method if <code>fast</code> is set to  <code>false</code>, skipped otherwise.</p> <p><pre><code>void bind(runtime::Task &amp;t_out, const int priority = -1);\n</code></pre> Add a fake input socket to the current task (see above <code>fake_input_sockets</code>  attribute) and binds it to the output <code>status</code> socket of the <code>t_out</code> task in  parameter. The new socket's <code>datatype</code> and <code>databytes</code> match the output  <code>status</code> socket of <code>t_out</code>. <code>fake_input_sockets</code> is always <code>fast</code>. This  method has to be manually called by the user.</p> <p><pre><code>size_t unbind(runtime::Task &amp;t_out);\n</code></pre> Unbinds and deletes the corresponding input socket in the  <code>fake_input_sockets</code> attribute. Can be called by <code>Sequence::set_n_frames()</code>  or manually by the user.</p> <p><pre><code>void reset();\n</code></pre> Resets the task's statistics. Not to be confused with <code>Module::reset()</code>. Manually called by the user.</p>"},{"location":"tests_advanced/","title":"Advanced","text":""},{"location":"tests_advanced/#generic-pipeline","title":"Generic Pipeline","text":"<p>This program allows to build and to execute generic chains directly from the  command line interface (CLI). Basically, there are 3 different types of task  that can be instantiated:</p> <ul> <li>First task: To be correct, a chain should start with a task that have no    input socket. As a consequence, there is only one first task and it is    possible to choose between <code>read</code> and <code>initialize</code> tasks.</li> <li>Middle task: A task that has an input and an output socket. It is up to    the user to decide the number and the combination of middle tasks he wants. It    is possible to select between <code>relay</code>, <code>relayf</code>, <code>increment</code> and <code>incrementf</code>    tasks.</li> <li>Last task: To be correct, a chain should always end with a task that does    not have an output socket. As a consequence, there is only one last task and    it is possible to choose between <code>write</code> or <code>finalize</code> tasks.</li> </ul> <p>Here is a summary of the available tasks and their behavior:</p> <ul> <li><code>read</code>: Reads data from a binary file and writes the read bytes on its output    socket.</li> <li><code>initialize</code> or <code>init</code>: Initializes the data in its output socket (useful for    benchmark and validation).</li> <li><code>relay</code>: Copies the data from its input socket into its output socket.</li> <li><code>relayf</code>: Variant of the <code>relay</code> task that uses    a forward socket, consequently, this task does NOTHING.</li> <li><code>increment</code> or <code>incr</code>: Increments (+1) the data of its input socket and writes    the result in its output socket.</li> <li><code>incrementf</code> or <code>incrf</code>: Variant of the <code>increment</code> task that uses    a forward socket to write the result in place.</li> <li><code>write</code>: Writes contents of its input socket into a binary file. It expects 0    or 1 values in its input socket to work correctly. </li> <li><code>finalize</code> or <code>fin</code>: Memorizes (= copies) the input data for further    validation (if there is a validation). </li> </ul> <p>There are three main ways of describing a processing chain:</p> <ol> <li> <p>Specification of homogeneous types of task per stage. This is performed     with the combination of the <code>-R</code> (or <code>--tsk-types-sta</code>) and <code>-n</code>     (or <code>--tsk-per-sta</code>) CLI parameters. <code>-R</code> gives the tasks type per stage     (example of a 4-stage pipeline: <code>-R (read,incr,relayf,write)</code>) and <code>-n</code> gives     how many tasks of the same type will be created per stage. For instance, the     combination of <code>-R (read,incr,relayf,write)</code> and <code>-n \"1,2,3,1\"</code> will produce     a 4-stage pipeline with the following sequence of tasks: <code>read</code> \\(\\rightarrow\\) <code>incr</code> \\(\\rightarrow\\) <code>incr</code> \\(\\rightarrow\\) <code>relayf</code> \\(\\rightarrow\\) <code>relayf</code> \\(\\rightarrow\\) <code>relayf</code> \\(\\rightarrow\\) <code>write</code>.</p> </li> <li> <p>Specification of heterogeneous types of task per stage. This is achieved     with <code>-r</code> (or <code>--tsk-types</code>) CLI parameter. For instance,     <code>-r ((init),(incrf,relay,incr),(fin))</code> will produce a 3-stage pipeline with     the following sequence of tasks: <code>init</code> \\(\\rightarrow\\) <code>incrf</code> \\(\\rightarrow\\) <code>relay</code> \\(\\rightarrow\\) <code>incr</code> \\(\\rightarrow\\) <code>fin</code>.</p> </li> <li> <p>Use of a scheduler to perform the pipeline decomposition in stages     automatically. This is achieved with <code>-C</code> (or <code>--chain</code>) CLI parameter.    For instance, <code>-C \"(init,relayf_15,incrementf_S_60,relay_15,fin)\"</code> defines    a chain that starts with an <code>initialize</code> task, after that, a <code>relayf</code> task of     15 microseconds is executed, then an <code>incrementf</code> task of 60 microseconds is     executed (note that the <code>_S</code> means that this task will be considered     sequential and \"non-replicable\" for the scheduler). Finally, a 15     microseconds <code>relay</code> task and a <code>finalize</code> task are executed. By default the     scheduler considers that the number of resources \\(R\\) is the number of CPU     hardware threads but you can override this behavior by using the <code>-t</code> (or     <code>--n-threads</code>) CLI parameter. It is also possible to choose the scheduler     algorithm through the <code>-S</code> (or <code>--sched</code>) CLI parameter. For now, the     available schedulers are <code>OTAC</code> and <code>FILE</code> (please see the note below about     the latest).</p> </li> </ol> <p>The first notation is a compressed way to describe chains of tasks. By default,  the chain is split in pipeline stages according to the given  decomposition (with <code>-R</code> and <code>-r</code>) and each stage is run on a separated thread.  It is also possible to run the chain in a sequence (with the <code>-q</code>  or <code>--force-sequence</code> CLI parameter). In this case, the given stage  decomposition is ignored and all the tasks of the chain are run by the same  thread. </p> <p>Note</p> <p>You cannot use <code>-r</code> and <code>-R</code> parameters at the same time, they are  exclusive.</p> <p>Note</p> <p>If <code>StreamPU</code> has been compiled with the CMake <code>-DSPU_LINK_HWLOC=ON</code> option, then it is possible to specify an pinning policy with  the <code>-P</code> or <code>--pinning-policy</code> CLI argument.</p> <p>Tip</p> <p>For the <code>initialize</code>, <code>increment</code>, <code>incrementf</code>, <code>relay</code>, <code>relayf</code> and  <code>finalize</code> tasks it is possible to specify the duration. For instance,  <code>relay_12</code> means that the <code>relay</code> task will spend 12 microseconds in active  waiting. This is different from using the <code>-s</code> CLI parameter. The <code>-s</code>  parameter will set the same duration for all the previously mentioned tasks.</p> <p>Note</p> <p>The scheduler <code>FILE</code> reads the scheduling from a JSON file, to set the path to this file there is the <code>-F</code> parameter (or <code>--sched-file</code>). The expected JSON file looks like the following: <pre><code>{\n\"platform\": \"x7ti\",\n\"resources\": {\n  \"p-core\": {\n    \"node-list\": [\"core0-5\"],\n    \"cluster-size\": 1,\n    \"smt\": 2\n  },\n  \"e-core\": {\n    \"node-list\": [\"core6-13\"],\n    \"cluster-size\": 4,\n    \"smt\": 1\n  }\n},\n\"scheduler_name\": \"HeRAD\",\n\"date\": \"2025-07-23\",\n\"schedule\": [\n    { \"tasks\": 5, \"threads\": 1, \"core-type\": \"p-core\", \"pinning-policy\": \"packed\",  \"sync_buff_size\": 1, \"sync_waiting_type\": \"active\"  },\n    { \"tasks\": 1, \"threads\": 1, \"core-type\": \"p-core\", \"pinning-policy\": \"packed\",  \"sync_buff_size\": 8, \"sync_waiting_type\": \"passive\" },\n    { \"tasks\": 6, \"threads\": 1, \"core-type\": \"p-core\", \"pinning-policy\": \"packed\",                                                      },\n    { \"tasks\": 4, \"threads\": 2, \"core-type\": \"p-core\", \"pinning-policy\": \"guided\",  \"sync_buff_size\": 1,                                },\n    { \"tasks\": 3, \"threads\": 7, \"core-type\": \"e-core\", \"pinning-policy\": \"distant\",                      \"sync_waiting_type\": \"active\"  },\n    { \"tasks\": 4, \"threads\": 2, \"core-type\": \"p-core\", \"pinning-policy\": \"guided\",                                                      }\n  ]\n}\n</code></pre> In the <code>schedule</code> field, each line corresponds to one pipeline stage, the  field <code>tasks</code> counts the number of consecutive tasks of the current stage  while the field <code>threads</code> gives the number of threads to use for the  current stage. Four policies are available while using the <code>pinning-policy</code>  field: <code>loose</code> (do not pin), <code>guided</code> (pin to core type), <code>packed</code> (pin  following the ascending order of core ids given by the <code>resources</code> field) or  <code>distant</code> (pin in a round robin way between the clusters and packages). When no pinning policy is specified, the <code>loose</code> policy is applied. Finally,  <code>sync_buff_size</code> and <code>sync_waiting_type</code> are optional and may help to fine  tune the synchronizations between the pipeline stages. The last stage should  not contain <code>sync_buff_size</code> or <code>sync_waiting_type</code> fields. Using the <code>FILE</code>  scheduler will override the following parameters: <code>-u</code> (or <code>--buffer-size</code>)  and <code>-w</code> (or <code>--active-waiting</code>).</p> <p>Moreover, for each stage it is possible to specify the number of replications  (= number of threads that will execute the stage) with the <code>-t</code>  (or <code>--n-threads</code>) CLI parameter. Here are some examples of generated pipelines:</p> 3-stage pipeline with in/out sockets3-stage pipeline with forward sockets3-stage pipeline with hybrid socketsComplex 5-stage pipeline <p> <code>test-generic-pipeline</code>: input/output sockets &amp; 3-stage pipeline. <pre><code>test-generic-pipeline -e 100 -n \"1,3,1\" -t \"3,1,3\" -R \"(init,increment,fin)\"\n</code></pre></p> <p> <code>test-generic-pipeline</code>: forward sockets &amp; 3-stage pipeline. <pre><code>test-generic-pipeline -i INPUT_FILE -n \"1,3,1\" -t \"1,3,1\" -R \"(read,relayf,write)\"\n</code></pre></p> <p> <code>test-generic-pipeline</code>: hybrid in/out and forward sockets &amp; 3-stage pipeline. <pre><code>test-generic-pipeline -i INPUT_FILE -t \"1,3,1\" -r \"((read),(relayf,incrementf,relay),(write))\"\n</code></pre></p> <p> <code>test-generic-pipeline</code>: hybrid in/out and forward sockets &amp; 5-stage pipeline. <pre><code>test-generic-pipeline -e 100 -t \"1,3,1,2,1\" -r \"((init,relayf,incr),(relayf,relay),(incrf),(relay),(relay,fin))\"\n</code></pre></p> <p>Command Line Arguments</p> <p>The following verbatim is a copy-paste from the <code>-h</code> stdout:</p> <pre><code>usage: ./bin/test-generic-pipeline [options]\n\n  -t, --n-threads          Number of threads to run in parallel for each stage                   [empty]\n  -f, --n-inter-frames     Number of frames to process in one task                               [1]\n  -s, --sleep-time         Sleep time duration in one task (microseconds)                        [5]\n  -d, --data-length        Size of data to process in one task (in bytes)                        [2048]\n  -e, --n-exec             Number of executions (0 means -&gt; never stop because of this counter)  [0]\n  -l, --n-exec-pro         Number of executions during the scheduler profiling phase             [100]\n  -u, --buffer-size        Size of the buffer between the different stages of the pipeline       [16]\n  -o, --dot-filepath       Path to dot output file                                               [empty]\n  -i, --in-filepath        Path to the input file (used to generate bits of the chain)           [empty]\n  -j, --out-filepath       Path to the output file (written at the end of the chain)             [\"file.out\"]\n  -c, --copy-mode          Enable to copy data in sequence (performance will be reduced)         [false]\n  -b, --step-by-step       Enable step-by-step sequence execution (performance will be reduced)  [false]\n  -p, --print-stats        Enable to print per task statistics (performance will be reduced)     [false]\n  -g, --debug              Enable task debug mode (print socket data)                            [false]\n  -q, --force-sequence     Force sequence instead of pipeline                                    [false]\n  -w, --active-waiting     Enable active waiting in the pipeline synchronizations                [false]\n  -n, --tsk-per-sta        The number of tasks on each stage of the pipeline                     [empty]\n  -r, --tsk-types          The socket type of each task (SFWD or SIO)                            [empty]\n  -R, --tsk-types-sta      The socket type of tasks on each stage (SFWD or SIO)                  [empty]\n  -C, --chain              Description of the tasks chain (to be combined with '-S' param)       [empty]\n  -S, --sched              Scheduler algorithm for the pipeline creation ('OTAC', 'FILE')        [\"OTAC\"]\n  -F, --sched-file         File that contains the scheduling, to combine with 'FILE' scheduler   [\"sched.json\"]\n  -v, --verbose            Show information about the scheduling choices                         [false]\n  -h, --help               This help                                                             [false]\n</code></pre>"},{"location":"tests_bootstrap/","title":"Bootstrap","text":"<p><code>StreamPU</code> comes with simple tests to validate its behavior. The later are always a good way to bootstrap when you want to write your first code with the DSEL. The source codes of the following tests are located in the  <code>tests/bootstrap</code> folder. Each file corresponds to an executable test.</p>"},{"location":"tests_bootstrap/#simple-chains","title":"Simple Chains","text":"<p>At some point we have to start somewhere :-). The following graphs are a very  simple chains made from <code>increment</code>/<code>incrementf</code> tasks that simply perform  \"\\(+1\\)\" on the data. Each time there are 6 <code>Incrementer</code> (\\(t_{[2:7]}\\)) so the  final expected values in the <code>Finalizer</code> (\\(t_8\\)) are equal to the values from  the <code>Initializer</code> (\\(t_1\\)) \"\\(+6\\)\".</p> With Input and Output SocketsWith Forward SocketsWith Input, Output and Forward Sockets <p> <pre><code>./bin/test-simple-chain -t 1\n</code></pre> This version of the simple chain is based on <code>increment</code> tasks that have one input socket and one output socket.</p> <p>Command Line Arguments</p> <p>The following verbatim is a copy-paste from the <code>-h</code> stdout:</p> <pre><code>usage: ./bin/test-simple-chain [options]\n\n  -t, --n-threads       Number of threads to run in parallel                                  [10]\n  -f, --n-inter-frames  Number of frames to process in one task                               [1]\n  -s, --sleep-time      Sleep time duration in one task (microseconds)                        [5]\n  -d, --data-length     Size of data to process in one task (in bytes)                        [2048]\n  -e, --n-exec          Number of sequence executions                                         [100000]\n  -o, --dot-filepath    Path to dot output file                                               [empty]\n  -c, --copy-mode       Enable to copy data in sequence (performance will be reduced)         [false]\n  -b, --step-by-step    Enable step-by-step sequence execution (performance will be reduced)  [false]\n  -p, --print-stats     Enable to print per task statistics (performance will be reduced)     [false]\n  -g, --debug           Enable task debug mode (print socket data)                            [false]\n  -u, --subseq          Enable subsequence in the executed sequence                           [false]\n  -v, --verbose         Enable verbose mode                                                   [false]\n  -h, --help            This help                                                             [false]\n</code></pre> <p> <pre><code>./bin/test-simple-chain-fwd -t 1\n</code></pre> This version of the simple chain is based on <code>incrementf</code> tasks that have only one forward socket.</p> <p>Command Line Arguments</p> <p>The following verbatim is a copy-paste from the <code>-h</code> stdout:</p> <pre><code>usage: ./bin/test-simple-chain-fwd [options]\n\n  -t, --n-threads       Number of threads to run in parallel                                  [10]\n  -f, --n-inter-frames  Number of frames to process in one task                               [1]\n  -s, --sleep-time      Sleep time duration in one task (microseconds)                        [5]\n  -d, --data-length     Size of data to process in one task (in bytes)                        [2048]\n  -e, --n-exec          Number of sequence executions                                         [100000]\n  -o, --dot-filepath    Path to dot output file                                               [empty]\n  -c, --copy-mode       Enable to copy data in sequence (performance will be reduced)         [true]\n  -b, --step-by-step    Enable step-by-step sequence execution (performance will be reduced)  [false]\n  -p, --print-stats     Enable to print per task statistics (performance will be reduced)     [false]\n  -g, --debug           Enable task debug mode (print socket data)                            [false]\n  -u, --subseq          Enable subsequence in the executed sequence                           [false]\n  -v, --verbose         Enable verbose mode                                                   [false]\n  -h, --help            This help                                                             [false]\n</code></pre> <p> <pre><code>./bin/test-simple-chain-hybrid -t 1\n</code></pre> This version of the simple chain is based on a combination of <code>increment</code>  and <code>incrementf</code> tasks.</p> <p>Command Line Arguments</p> <p>The following verbatim is a copy-paste from the <code>-h</code> stdout:</p> <pre><code>usage: ./bin/test-simple-chain-hybrid [options]\n\n  -t, --n-threads       Number of threads to run in parallel                                  [10]\n  -f, --n-inter-frames  Number of frames to process in one task                               [1]\n  -s, --sleep-time      Sleep time duration in one task (microseconds)                        [5]\n  -d, --data-length     Size of data to process in one task (in bytes)                        [2048]\n  -e, --n-exec          Number of sequence executions                                         [100000]\n  -o, --dot-filepath    Path to dot output file                                               [empty]\n  -c, --copy-mode       Enable to copy data in sequence (performance will be reduced)         [false]\n  -b, --step-by-step    Enable step-by-step sequence execution (performance will be reduced)  [false]\n  -p, --print-stats     Enable to print per task statistics (performance will be reduced)     [false]\n  -g, --debug           Enable task debug mode (print socket data)                            [false]\n  -v, --verbose         Enable verbose mode                                                   [false]\n  -h, --help            This help                                                             [false]\n</code></pre>"},{"location":"tests_bootstrap/#looping","title":"Looping","text":"<p>The looping tests are here to introduce and to validate how to implement simple control flow. Each of the graph below execute one or multiple loops and in the innermost loop 6 <code>increment</code> tasks are executed. <code>Switcher</code> modules are used to  create two different paths, one for the loop and the other for the exit (the  <code>Finalizer</code> task here). The output socket of the <code>iterate</code> task (from the  <code>Iterator</code>  module) is bound to the input socket 1 of the <code>commute</code> task. The  <code>iterate</code> task controls if the commute should execute the 6 <code>increment</code> tasks or  the end the stream by executing the <code>finalize</code> task.</p> <p>Each time there are 6 <code>Incrementer</code> in the innermost loop so the final expected  values in the <code>Finalizer</code> are equal to the values from the <code>Initializer</code> (\\(t_1\\))  \"\\(+(i \\times j\\times 6)\\)\" (\\(j = 1\\) in the <code>for</code> loop and <code>do-while</code> loop  examples). </p> <code>For</code> Loop<code>Do-while</code> LoopNested Loops <p> <pre><code>./bin/test-for-loop -t 1 -i 10\n</code></pre> This test implements a classic <code>for-loop</code> where the condition is evaluated first in \\(t_3\\) (a basic loop counter) and the the body of the loop is  executed (\\(t_{[5:10]}\\)) after. Note that this <code>for-loop</code> example can easily be extended to a more generic <code>while-loop</code> if the \\(t_3\\) task is replaced  by an other task that depends on an input data socket. </p> <p>In the command line, <code>-i 10</code> indicates that the loop is repeated 10 times.</p> <p>Command Line Arguments</p> <p>The following verbatim is a copy-paste from the <code>-h</code> stdout:</p> <pre><code>usage: ./bin/test-for-loop [options]\n\n  -t, --n-threads       Number of threads to run in parallel                                  [10]\n  -f, --n-inter-frames  Number of frames to process in one task                               [1]\n  -s, --sleep-time      Sleep time duration in one task (microseconds)                        [5]\n  -d, --data-length     Size of data to process in one task (in bytes)                        [2048]\n  -e, --n-exec          Number of sequence executions                                         [100000]\n  -i, --n-loop          Number of iterations to perform in the loop                           [10]\n  -o, --dot-filepath    Path to dot output file                                               [empty]\n  -c, --copy-mode       Enable to copy data in sequence (performance will be reduced)         [false]\n  -b, --step-by-step    Enable step-by-step sequence execution (performance will be reduced)  [false]\n  -p, --print-stats     Enable to print per task statistics (performance will be reduced)     [false]\n  -g, --debug           Enable task debug mode (print socket data)                            [false]\n  -h, --help            This help                                                             [false]\n</code></pre> <p> <pre><code>./bin/test-do-while-loop -t 1 -i 10\n</code></pre> An implementation of a <code>do-while</code> loop where the condition \\(t_9\\) is  evaluated after the body of the loop (\\(t_{[3:8]}\\)).</p> <p>In the command line, <code>-i 10</code> indicates that the loop is repeated 10 times.</p> <p>Command Line Arguments</p> <p>The following verbatim is a copy-paste from the <code>-h</code> stdout:</p> <pre><code>usage: ./bin/test-do-while-loop [options]\n\n  -t, --n-threads       Number of threads to run in parallel                                  [10]\n  -f, --n-inter-frames  Number of frames to process in one task                               [1]\n  -s, --sleep-time      Sleep time duration in one task (microseconds)                        [5]\n  -d, --data-length     Size of data to process in one task (in bytes)                        [2048]\n  -e, --n-exec          Number of sequence executions                                         [100000]\n  -i, --n-loop          Number of iterations to perform in the loop                           [9]\n  -o, --dot-filepath    Path to dot output file                                               [empty]\n  -c, --copy-mode       Enable to copy data in sequence (performance will be reduced)         [false]\n  -b, --step-by-step    Enable step-by-step sequence execution (performance will be reduced)  [false]\n  -p, --print-stats     Enable to print per task statistics (performance will be reduced)     [false]\n  -g, --debug           Enable task debug mode (print socket data)                            [false]\n  -h, --help            This help                                                             [false]\n</code></pre> <p> </p> <pre><code>./bin/test-do-while-loop -t 1 -i 5 -j 2\n</code></pre> <p>Implementation of 2 nested <code>for-loop</code>s. <code>-j 2</code> controls the number of times the innermost loops is repeated and <code>-i 5</code> controls the outermost loop. </p> <p>Command Line Arguments</p> <p>The following verbatim is a copy-paste from the <code>-h</code> stdout:</p> <pre><code>usage: ./bin/test-nested-loops [options]\n\n  -t, --n-threads       Number of threads to run in parallel                                  [10]\n  -f, --n-inter-frames  Number of frames to process in one task                               [1]\n  -s, --sleep-time      Sleep time duration in one task (microseconds)                        [5]\n  -d, --data-length     Size of data to process in one task (in bytes)                        [2048]\n  -e, --n-exec          Number of sequence executions                                         [100000]\n  -i, --n-loop-out      Number of iterations to perform in the outer loop                     [5]\n  -j, --n-loop-in       Number of iterations to perform in the inner loop                     [2]\n  -o, --dot-filepath    Path to dot output file                                               [empty]\n  -c, --copy-mode       Enable to copy data in sequence (performance will be reduced)         [false]\n  -b, --step-by-step    Enable step-by-step sequence execution (performance will be reduced)  [false]\n  -p, --print-stats     Enable to print per task statistics (performance will be reduced)     [false]\n  -g, --debug           Enable task debug mode (print socket data)                            [false]\n  -h, --help            This help                                                             [false]\n</code></pre>"},{"location":"tests_bootstrap/#switch-case","title":"Switch-case","text":"<p>The following test implements a <code>switch-case</code> pattern based on <code>increment</code>  tasks (\\(t_{[4;9]}\\)). Depending on the <code>control</code> task (\\(t_2\\)), one of the three  different paths (or cases in the <code>switch-case</code> pattern) will be executed. The  first path (or <code>case 0</code>) is composed by \\(t_4\\), \\(t_5\\) and \\(t_6\\) tasks, the second  path (or <code>case 1</code>) is composed by \\(t_7\\) and \\(t_8\\) tasks while the  third path (or <code>case 2</code>) is only composed by the \\(t_9\\) task.</p> <p>The final expected values in the <code>Finalizer</code> depends on the selected path. If  the first path is chosen (\\(t_{[4:6]}\\)), \"\\(+3\\)\" is added to the values from the  <code>Initializer</code> (\\(t_1\\)). If the second path is chosen (\\(t_{[7:8]}\\)), \"\\(+2\\)\" is  added to the values from the <code>Initializer</code> (\\(t_1\\)). Finally, if the last path is  chosen (\\(t_{9}\\)), \"\\(+1\\)\" is added to the values from the <code>Initializer</code> (\\(t_1\\)).</p> <p></p> <pre><code>./bin/test-exclusive-paths -t 1 -y\n</code></pre> <p>The <code>-y</code> option indicates that the <code>Controller</code> is cyclic: for the first stream the first path is selected, for the second stream the second path is taken, for the third stream the third path is taken, for the fourth stream the first path is taken and so on...</p> <p>It is also possible to have a fixed path for all the streams with the <code>-a N</code>  option (with <code>N</code> the path id).</p> <p>Note</p> <p>You cannot use <code>-y</code> and <code>-a</code> parameters at the same time, they are  exclusive.</p> <p>Command Line Arguments</p> <p>The following verbatim is a copy-paste from the <code>-h</code> stdout:</p> <pre><code>usage: ./bin/test-exclusive-paths [options]\n\n  -t, --n-threads       Number of threads to run in parallel                                  [10]\n  -f, --n-inter-frames  Number of frames to process in one task                               [1]\n  -s, --sleep-time      Sleep time duration in one task (microseconds)                        [5]\n  -d, --data-length     Size of data to process in one task (in bytes)                        [2048]\n  -e, --n-exec          Number of sequence executions                                         [100000]\n  -a, --path            Path to take in the switch (0, 1 or 2)                                [0]\n  -o, --dot-filepath    Path to dot output file                                               [empty]\n  -c, --copy-mode       Enable to copy data in sequence (performance will be reduced)         [false]\n  -b, --step-by-step    Enable step-by-step sequence execution (performance will be reduced)  [false]\n  -p, --print-stats     Enable to print per task statistics (performance will be reduced)     [false]\n  -g, --debug           Enable task debug mode (print socket data)                            [false]\n  -y, --cyclic-path     Enable cyclic selection of the path (with this `--path` is ignored)   [false]\n  -h, --help            This help                                                             [false]\n</code></pre>"},{"location":"tests_bootstrap/#simple-pipeline","title":"Simple Pipeline","text":"<p>This test is an implementation of a 3 stages pipeline. The first stage  \\(S1 = t_1\\) reads data from a file. Thus, the <code>generate</code> task (\\(t_1\\)) is  intrinsically sequential (= executes on a single thread). The second stage  \\(S2 = t_{[2:7]}\\) just copies the data from the stage \\(S_1\\) through <code>relay</code>  tasks. The <code>relay</code> tasks can be replicated and ran over multiple threads.  Finally, the last stage \\(S3 = t_8\\) is a <code>Sink</code> that writes the data on the  file system. This operation is also intrinsically sequential.</p> <p>The purpose of this test is to validate that buffer exchanges between the  pipeline stages are working well. At the end, the code checks that the input  file and the output file are the same.</p> <p></p> <pre><code>./bin/test-simple-pipeline -t 3 --in-filepath ~/.bashrc --out-filepath output_file\n</code></pre> <p>Note</p> <p>The second output socket of the <code>generate</code> task (\\(t_1\\)) is bound to the second input socket of the <code>send_count</code> task (\\(t_8\\)). If the read data is  not a multiple of the stream size, then the stream is padded with 0. But, these zeros are NOT written on the file system by the <code>send_count</code> task. The second output socket of the <code>generate</code> task (\\(t_1\\)) contains the number of bits that have been read from the input file. Then, the <code>send_count</code> task can avoid to write the padding zeros (if any).</p> <p>Command Line Arguments</p> <p>The following verbatim is a copy-paste from the <code>-h</code> stdout:</p> <pre><code>usage: ./bin/test-simple-pipeline [options]\n\n  -t, --n-threads       Number of threads to run in parallel                                  [10]\n  -f, --n-inter-frames  Number of frames to process in one task                               [1]\n  -s, --sleep-time      Sleep time duration in one task (microseconds)                        [5]\n  -d, --data-length     Size of data to process in one task (in bytes)                        [2048]\n  -u, --buffer-size     Size of the buffer between the different stages of the pipeline       [2048]\n  -o, --dot-filepath    Path to dot output file                                               [empty]\n  -i, --in-filepath     Path to the input file (used to generate bits of the chain)           [empty]\n  -j, --out-filepath    Path to the output file (written at the end of the chain)             [\"file.out\"]\n  -c, --copy-mode       Enable to copy data in sequence (performance will be reduced)         [false]\n  -b, --step-by-step    Enable step-by-step sequence execution (performance will be reduced)  [false]\n  -p, --print-stats     Enable to print per task statistics (performance will be reduced)     [false]\n  -g, --debug           Enable task debug mode (print socket data)                            [false]\n  -q, --force-sequence  Force sequence instead of pipeline                                    [false]\n  -w, --active-waiting  Enable active waiting in the pipeline synchronizations                [false]\n  -h, --help            This help                                                             [false]\n</code></pre>"},{"location":"thread_pinning/","title":"Thread Pinning","text":"<p><code>StreamPU</code> enables to select on which process units (PUs) the threads are  effectively run. This is called thread pinning and it can significantly  benefit to the performance, especially on modern heterogeneous architectures.  To do so, the runtime relies on the  <code>hwloc</code> library.</p> <p>Warning</p> <p>To use thread pinning, <code>hwloc</code> library has to be installed on the system and <code>StreamPU</code> needs to be compiled with the <code>SPU_HWLOC</code> preprocessor  definition. It can simply be achieved using the following CMake option: <pre><code>cmake .. -DSPU_LINK_HWLOC=ON\n</code></pre> If <code>StreamPU</code> is not linked with the <code>hwloc</code> library, then the thread  pinning interface will have no effect and the threads will not be pinned.</p> <p>Info</p> <p>Thread pinning relies the OS. The later needs to expose appropriated system  calls. While Linux and Windows provide these syscalls,  macOS does not...  Thus, thread pinning will have no effect on macOS :-(.</p>"},{"location":"thread_pinning/#portable-hardware-locality","title":"Portable Hardware Locality","text":"<p>Portable Hardware Locality (<code>hwloc</code> in short) is a library which provides a  portable abstraction of the hierarchical topology of modern  architectures (see the figure below).</p> <p> </p>      Result of the hwloc-ls command on the Orange Pi 5 Plus board (Rockchip      RK3588 SoC, cores 0-3 are energy efficient ARM Cortex-A55 and cores 4-7 are      powerful ARM Cortex-A76).    <p><code>hwloc</code> gives the ability to pin threads over various level of hierarchy  represented by a tree structure. The deepest/lowest nodes (the leaves) are the  PUs while higher nodes represent sets of PUs that are physically close. For  instance, a PUs set can share the same UMA node (in the case of a NUMA  architecture), the same LLC or the same package. </p> <p>In the Orange Pi 5 SBC, if we pin a thread on the <code>Package L#0</code>, it will run  over the following set of PUs: <code>PU L#0</code>, <code>PU L#1</code>, <code>PU L#2</code> and <code>PU L#3</code>.  Thus, the pinned thread can move in the selected <code>hwloc</code> node during the  execution and it is up to the OS to schedule the thread on the selected PUs  set.</p> <p>Warning</p> <p>The indexes given by <code>hwloc</code> can be different from those given by the OS:  they are logical indexes that express the real locality. Consequently, in  <code>StreamPU</code>, it is important to use <code>hwloc</code> logical indexes. The  <code>hwloc-ls</code> command gives an overview of the current topology with these  logical indexes.</p>"},{"location":"thread_pinning/#sequence-pipeline","title":"Sequence &amp; Pipeline","text":"<p>In <code>StreamPU</code>, thread pinning can be set in <code>runtime::Sequence</code> and  <code>runtime::Pipeline</code> class constructors. In both cases, there is a dedicated  argument of <code>std::string</code> type named <code>sequence_pinning_policy</code> for  <code>runtime::Sequence</code> or <code>pipeline_pinning_policy</code> for <code>runtime::Pipeline</code>.</p> <p>Info</p> <p>For NUMA architectures, it is important to specify thread pinning at the  construction of the <code>runtime::Sequence</code>/<code>runtime::Pipeline</code> object to  guarantee that the data will be allocated and initialized on the right  memory banks (according to the first touch policy) during the replication  process.</p> <p>To specify the pinning policy, we defined a syntax to express <code>hwloc</code> objects  with three different separators:  </p> <ul> <li>Pipeline stage (does not concern <code>runtime::Sequence</code>): <code>|</code></li> <li>Replicated stage (= replicated sequence = one thread): <code>;</code> </li> <li>For one thread, the list of pinned <code>hwloc</code> objects (= logical or): <code>,</code> </li> </ul> <p>Then, the pinning policy can contains all the available <code>hwloc</code> objects. Below  is the correspondence between the <code>std::string</code> and the <code>hwloc</code> object types:</p> <pre><code>std::map&lt;std::string, hwloc_obj_type_t&gt; str_to_hwloc_obj =\n{ \n  /* global containers */             /* data caches */              /* instruction caches */\n  { \"GROUP\",   HWLOC_OBJ_GROUP    },  { \"L5D\", HWLOC_OBJ_L5CACHE },  { \"L3I\",  HWLOC_OBJ_L3ICACHE },\n  { \"NUMA\",    HWLOC_OBJ_NUMANODE },  { \"L4D\", HWLOC_OBJ_L4CACHE },  { \"L2I\",  HWLOC_OBJ_L2ICACHE },\n  { \"PACKAGE\", HWLOC_OBJ_PACKAGE  },  { \"L3D\", HWLOC_OBJ_L3CACHE },  { \"L1I\",  HWLOC_OBJ_L1ICACHE },\n                                      { \"L2D\", HWLOC_OBJ_L2CACHE },  /* compute units */\n                                      { \"L1D\", HWLOC_OBJ_L1CACHE },  { \"CORE\", HWLOC_OBJ_CORE     },\n                                                                     { \"PU\",   HWLOC_OBJ_PU       },\n};           \n</code></pre> <p>To specify the index <code>X</code> of an <code>hwloc</code> object, the following syntax is used:  <code>OBJECT_X</code> (ex: <code>PU_5</code> refers to the logical PU n\u00b05).</p> <p>Info</p> <p><code>CORE</code> and <code>PU</code> objects can be confusing. If the CPU cores do not support SMT, then <code>CORE</code> and <code>PU</code> are the same. However, if the CPU cores support SMT, then the <code>PU</code> is the hardware thread identifier inside a given <code>CORE</code>.</p>"},{"location":"thread_pinning/#illustrative-examples","title":"Illustrative Examples","text":"<p>This section gives some examples to understand how the syntax works. We  suppose that we have a CPU with 8 PUs with the same topology as the the Orange  Pi 5 Plus SBC presented before.</p>"},{"location":"thread_pinning/#example-1","title":"Example 1","text":"<p>Let's suppose we want to setup a 3-stage pipeline with the following  characteristics:</p> <ul> <li>Stage 1 - No replication (= 1 thread): <ul> <li>Pinned to <code>PU_0</code></li> </ul> </li> <li>Stage 2 - 4 replications (= 4 threads): <ul> <li>Thread n\u00b01 is pinned to <code>PU_4</code> or <code>PU_5</code></li> <li>Thread n\u00b02 is pinned to <code>PU_4</code> or <code>PU_5</code></li> <li>Thread n\u00b03 is pinned to <code>PU_6</code> or <code>PU_7</code></li> <li>Thread n\u00b04 is pinned to <code>PU_6</code> or <code>PU_7</code></li> </ul> </li> <li>Stage 3 -  No replication (= 1 thread): <ul> <li>Pinned to <code>PU_0</code>, <code>PU_1</code>, <code>PU_2</code> or <code>PU_3</code></li> </ul> </li> </ul> <pre><code>graph LR;\nS1T1(Stage 1, thread 1 - pin: PU_0)--&gt;SYNC1;\nSYNC1(Sync)--&gt;S2T1;\nSYNC1(Sync)--&gt;S2T2;\nSYNC1(Sync)--&gt;S2T3;\nSYNC1(Sync)--&gt;S2T4;\nS2T1(Stage 2, thread 1 - pin: PU_4 or PU_5)--&gt;SYNC2;\nS2T2(Stage 2, thread 2 - pin: PU_4 or PU_5)--&gt;SYNC2;\nS2T3(Stage 2, thread 3 - pin: PU_6 or PU_7)--&gt;SYNC2;\nS2T4(Stage 2, thread 4 - pin: PU_6 or PU_7)--&gt;SYNC2;\nSYNC2(Sync)--&gt;S3T1(Stage 3, thread 1 - pin: PU_0, PU_1, PU_2 or PU_3);</code></pre> <p>In the previous configuration, 6 threads will execute simultaneously (even if  the given architecture supports up to 8 executions in parallel).</p> <p>To instantiate this <code>runtime::Pipeline</code>, here are the corresponding constructor  parameters:  </p> <ul> <li>Number of replications (= threads) per stage: <code>{ 1, 4, 1 }</code></li> <li>Enabling pinning per stage: <code>{ true, true, true }</code> </li> <li>Pinning policy:    <code>\"PU_0 | PU_4, PU_5; PU_4, PU_5; PU_6, PU_7; PU_6, PU_7 | PU_0, PU_1, PU_2, PU_3\"</code></li> </ul> <p>The previous pinning policy syntax can be compressed a little bit as follow:</p> <ul> <li>Pinning policy :    <code>\"PU_0 | PACKAGE_1; PACKAGE_1; PACKAGE_2; PACKAGE_2 | PACKAGE_0\"</code></li> </ul>"},{"location":"thread_pinning/#example-2","title":"Example 2","text":"<p>Let's now consider that we want to pin all the threads of the stage 2 on the  <code>PU_4</code>, <code>PU_5</code>, <code>PU_6</code> or <code>PU_7</code> (this is less restrictive than the previous  example). The pinning strategy for stage 1 and 3 is unchanged.</p> <pre><code>graph LR;\nS1T1(Stage 1, thread 1 - pin: PU_0)--&gt;SYNC1;\nSYNC1(Sync)--&gt;S2T1;\nSYNC1(Sync)--&gt;S2T2;\nSYNC1(Sync)--&gt;S2T3;\nSYNC1(Sync)--&gt;S2T4;\nS2T1(Stage 2, thread 1 - pin: PU_4, PU_5, PU_6 or PU_7)--&gt;SYNC2;\nS2T2(Stage 2, thread 2 - pin: PU_4, PU_5, PU_6 or PU_7)--&gt;SYNC2;\nS2T3(Stage 2, thread 3 - pin: PU_4, PU_5, PU_6 or PU_7)--&gt;SYNC2;\nS2T4(Stage 2, thread 4 - pin: PU_4, PU_5, PU_6 or PU_7)--&gt;SYNC2;\nSYNC2(Sync)--&gt;S3T1(Stage 3, thread 1 - pin: PU_0, PU_1, PU_2 or PU_3);</code></pre> <p>Here are the corresponding parameters: </p> <ul> <li>Number of replications (= threads) per stage: <code>{ 1, 4, 1 }</code></li> <li>Enabling pinning per stage: <code>{ true, true, true }</code> </li> <li>Pinning policy : <code>\"PU_0 | PACKAGE_1, PACKAGE_2 | PACKAGE_0\"</code></li> </ul> <p>With the previous syntax, the 4 threads of the stage 2 will apply the  <code>PACKAGE_1, PACKAGE_2</code> policy.</p>"},{"location":"thread_pinning/#example-3","title":"Example 3","text":"<p>It is also possible to choose the stages we want to pin or not using a vector of  <code>boolean</code>. Let's suppose we do not want to specify any pinning for the stage 1. </p> <pre><code>graph LR;\nS1T1(Stage 1, thread 1 - no pinning)--&gt;SYNC1;\nSYNC1(Sync)--&gt;S2T1;\nSYNC1(Sync)--&gt;S2T2;\nSYNC1(Sync)--&gt;S2T3;\nSYNC1(Sync)--&gt;S2T4;\nS2T1(Stage 2, thread 1 - pin: PU_4, PU_5, PU_6 or PU_7)--&gt;SYNC2;\nS2T2(Stage 2, thread 2 - pin: PU_4, PU_5, PU_6 or PU_7)--&gt;SYNC2;\nS2T3(Stage 2, thread 3 - pin: PU_4, PU_5, PU_6 or PU_7)--&gt;SYNC2;\nS2T4(Stage 2, thread 4 - pin: PU_4, PU_5, PU_6 or PU_7)--&gt;SYNC2;\nSYNC2(Sync)--&gt;S3T1(Stage 3, thread 1 - pin: PU_0, PU_1, PU_2 or PU_3);</code></pre> <p>Here are the corresponding parameters:</p> <ul> <li>Number of replications (= threads) per stage: <code>{ 1, 4, 1 }</code></li> <li>Enabling pinning per stage: <code>{false, true, true}</code> </li> <li>Pinning policy: <code>\"| PACKAGE_1, PACKAGE_2 | PACKAGE_0\"</code></li> </ul> <p>In this case, the OS will be in charge of pinning the thread of the first stage.</p>"},{"location":"thread_pinning/#unpin","title":"Unpin","text":"<p>An unpin function exists and can be called by each thread individually. Once  the <code>unpin</code> function is triggered the thread will be free to be scheduled by the  OS over all the process units.</p> <p>Warning</p> <p>We assume that the user is aware of the computer architecture, uses the  logical indexes of <code>hwloc</code> and follows the previously defined syntax rules,  otherwise the code will throw exceptions.</p>"},{"location":"wip/","title":"Work in Progress","text":""},{"location":"wip/#forward-socket-with-control-flow","title":"Forward Socket with Control Flow","text":"<p>Currently when <code>gen_processes()</code> is called, <code>select</code> and <code>commute</code> tasks do not  behave properly with <code>SFWD</code> tasks bound directly on their data sockets. Indeed  they only rebind the first bound sockets while they should use the <code>explore_thread_rec()</code> function to update the  data pointers of every linked <code>SFWD</code>. For now, tasks bound to switchers should  be <code>SIO</code>.</p>"},{"location":"wip/#task-ids-update","title":"Task IDs Update","text":"<p>When <code>push</code> and <code>pull</code> tasks are inserted into the pipeline the  <code>update_tasks_id()</code> method is called to assert that the task IDs are still  coherent, the task with the greatest ID is used to insert the <code>push</code> task in  stages for instance. It uses the following algorithm.</p> <pre><code># It is called with min_id = 0 in the pipeline code, min_id is shared between \n# recursive calls and NOT copied because it is a reference\nvoid update_tasks_id(Node n, min_id):\n    mark(n)\n    for every task t in n:\n        t.set_id(min_id)\n        min_id += 1\n    for every child c of n that is not marked:\n        update_tasks_id(c, min_id)\n</code></pre> <p>This however can lead to issues. For instance when updating the IDs of a sequence with a switch-case.</p> <p>Warning</p> <p>The following is a graph of tasks, and NOT a graph of sub-sequences.</p> <pre><code>    graph LR;\n    A[Task_first ID : 0]--&gt;B[Task Commute ID : 1];\n    B[Task Commute ID : 1]-.-&gt;C[Task_b1 ID : 2];\n    B[Task Commute ID : 1]-.-&gt;D[Task_b2 ID : 3];\n    B[Task Commute ID : 1]-.-&gt;E[Task_b3 ID : 4];\n    C[Task_b1 ID : 2]-.-&gt;F[Task select ID : 5];\n    D[Task_b2 ID : 3]-.-&gt;F[Task select ID : 5];\n    E[Task_b3 ID : 4]-.-&gt;F[Task select ID : 5];\n    F[Task select ID : 5]--&gt;G[Task_last ID : 6];</code></pre> <p>Here is the result of the DFS-id update.</p> <pre><code>    graph LR;\n    A[Task_first ID : 0]--&gt;B[Task Commute ID : 1];\n    B[Task Commute ID : 1]-.-&gt;C[Task_b1 ID : 2];\n    B[Task Commute ID : 1]-.-&gt;D[Task_b2 ID : 5];\n    B[Task Commute ID : 1]-.-&gt;E[Task_b3 ID : 6];\n    C[Task_b1 ID : 2]-.-&gt;F[Task select ID : 3];\n    D[Task_b2 ID : 5]-.-&gt;F[Task select ID : 3];\n    E[Task_b3 ID : 6]-.-&gt;F[Task select ID : 3];\n    F[Task select ID : 3]--&gt;G[Task_last ID : 4];</code></pre> <p>This is not a coherent set of IDs, <code>Task_last</code> should not have an ID inferior than <code>Task_b2</code> and <code>Task_b3</code>, but due to the nature of the DFS algorithm it was assigned a lesser id because it was reached first, thus <code>Task_b2</code> was updated later and given a greater id.</p> <p>The current implementation uses the following algorithm.</p> <pre><code># We no longer have a minimum, instead we simply increment each ID by 1\nvoid update_tasks_id(Node n):\n    mark(n)\n    for every task t in n:\n        t.set_id(t.id + 1)\n    for every child c of n that is not marked:\n        update_tasks_id(c)\n</code></pre> <p>Which results in</p> <pre><code>    graph LR;\n    A[Task_first ID : 1]--&gt;B[Task Commute ID : 2];\n    B[Task Commute ID : 2]-.-&gt;C[Task_b1 ID : 3];\n    B[Task Commute ID : 2]-.-&gt;D[Task_b2 ID : 4];\n    B[Task Commute ID : 2]-.-&gt;E[Task_b3 ID : 5];\n    C[Task_b1 ID : 3]-.-&gt;F[Task select ID : 6];\n    D[Task_b2 ID : 4]-.-&gt;F[Task select ID : 6];\n    E[Task_b3 ID : 5]-.-&gt;F[Task select ID : 6];\n    F[Task select ID : 6]--&gt;G[Task_last ID : 7];</code></pre> <p>While this is a coherent set of IDs, we lost control over the <code>min_id</code>, which is less-than-ideal and thus every task has a new ID while they didn't need change. A better long term solution would perhaps be to use the <code>min_id</code> system in order to keep the old numerical values of already existing tasks but this time with a breadth-first type algorithm.</p> <p></p>"},{"location":"wip/#end-of-sequence-commuteselect","title":"End-of-sequence (<code>commute</code>/<code>select</code>)","text":"<p>Currently a sequence may not end with a switcher task (<code>commute</code> or <code>select</code>),  this is problematic for pipelines as this means that individual stages cannot  have a commute as their last task. This would require modifications to the  <code>last_subsequence()</code> method, as an end-of-sequence commute would still have a  children.</p>"},{"location":"wip/#example","title":"Example","text":"<p>As an example, let's try to divide the following sequence into 3 stages</p> <pre><code>graph LR;\nA(SS 1)-.-&gt;B(SS select);\nB(SS select)--&gt;C(SS 2);\nC(SS 2)--&gt;F(SS commute);\nF(SS commute)-.-&gt;E(SS 3);\nE(SS 3)-.-&gt;B(SS select);\n    F(SS commute)-.-&gt;G(SS 4);</code></pre> Stage 1Stage 2Stage 3 <pre><code>graph LR;\nA(SS1);</code></pre> <pre><code>graph LR;\nB(SS select)--&gt;C(SS 2);\nC(SS 2)--&gt;F(SS commute);\nF(SS commute)-.-&gt;E(SS 3);\nE(SS 3)-.-&gt;B(SS select);</code></pre> <pre><code>graph LR;\nG(SS 4);</code></pre> <p>With our current implementation of the DFS, Stage 2 technically has no final sub-sequence as every single node has at least one child, thus making the  insertion of push tasks impossible. A solution would be to introduce a fake task  after each commute on their last path during pipeline creations.</p>"}]}